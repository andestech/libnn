<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Andes Neural Network Library User Manual: Softmax Functions</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Andes Neural Network Library User Manual
   &#160;<span id="projectnumber">version3.3</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('group__Softmax.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="summary">
<a href="#func-members">Functions</a>  </div>
  <div class="headertitle">
<div class="title">Softmax Functions</div>  </div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:ga8a6aec663c02f68d170a1f68a2b94b99"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__Softmax.html#ga8a6aec663c02f68d170a1f68a2b94b99">riscv_nn_softmax_s8_fast</a> (const q7_t *in_vec, const uint16_t size, q7_t *out_vec)</td></tr>
<tr class="separator:ga8a6aec663c02f68d170a1f68a2b94b99"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gac4143da954d1f2ab48427022ef51c56e"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__Softmax.html#gac4143da954d1f2ab48427022ef51c56e">riscv_nn_softmax_s16_fast</a> (const q15_t *in_vec, const uint16_t size, q15_t *out_vec)</td></tr>
<tr class="separator:gac4143da954d1f2ab48427022ef51c56e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga7cf0ecef9eef8cf2a773574d071abea7"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__Softmax.html#ga7cf0ecef9eef8cf2a773574d071abea7">riscv_nn_softmax_s8_hp</a> (const int8_t *in_tensor, const int32_t in_tensor_row, const int32_t in_tensor_col, const int32_t scale, const int32_t lshift, const int32_t diff_min, int8_t *out_tensor)</td></tr>
<tr class="separator:ga7cf0ecef9eef8cf2a773574d071abea7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga1a4a512687ba8a9f42f5b416abd6903b"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__Softmax.html#ga1a4a512687ba8a9f42f5b416abd6903b">riscv_nn_softmax_s8_s16_hp</a> (const int8_t *in_tensor, const int32_t in_tensor_row, const int32_t in_tensor_col, const int32_t scale, const int32_t lshift, const int32_t diff_min, int16_t *out_tensor)</td></tr>
<tr class="separator:ga1a4a512687ba8a9f42f5b416abd6903b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaeb8a9b8dd55434f4df1361ab320b8e9f"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__Softmax.html#gaeb8a9b8dd55434f4df1361ab320b8e9f">riscv_nn_softmax_u8_hp</a> (const uint8_t *in_tensor, const int32_t in_tensor_row, const int32_t in_tensor_col, const int32_t scale, const int32_t lshift, const int32_t diff_min, uint8_t *out_tensor)</td></tr>
<tr class="separator:gaeb8a9b8dd55434f4df1361ab320b8e9f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga1edc59ec814a19dc7055420c7831912d"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__Softmax.html#ga1edc59ec814a19dc7055420c7831912d">riscv_nn_softmax_s16_hp</a> (const int16_t *in_tensor, const int32_t in_tensor_row, const int32_t in_tensor_col, const int32_t scale, const int32_t shift, const int16_t *exp_lut, const int16_t *one_by_one_lut, int16_t *out_tensor)</td></tr>
<tr class="separator:ga1edc59ec814a19dc7055420c7831912d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaafecfd2ae73cbb1fc175038dbd565204"><td class="memItemLeft" align="right" valign="top">int32_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__Softmax.html#gaafecfd2ae73cbb1fc175038dbd565204">riscv_nn_softmax_f32</a> (const float32_t *in_vec, const uint32_t size, float32_t *out_vec)</td></tr>
<tr class="separator:gaafecfd2ae73cbb1fc175038dbd565204"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga95f34f7382bc22ca80386f52e99624b7"><td class="memItemLeft" align="right" valign="top">int32_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__Softmax.html#ga95f34f7382bc22ca80386f52e99624b7">riscv_nn_softmax_f32_2pass</a> (const float32_t *in_vec, const uint32_t size, float32_t *out_vec)</td></tr>
<tr class="separator:ga95f34f7382bc22ca80386f52e99624b7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga8ae830fce2d3b3a7ca70bac52828f6df"><td class="memItemLeft" align="right" valign="top">int32_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__Softmax.html#ga8ae830fce2d3b3a7ca70bac52828f6df">riscv_nn_softmax2d_f32</a> (const float32_t *in_buf, uint32_t row, uint32_t col, float32_t *out_buf, float32_t *tmp_buf)</td></tr>
<tr class="separator:ga8ae830fce2d3b3a7ca70bac52828f6df"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<h2 class="groupheader">Function Documentation</h2>
<a id="ga8a6aec663c02f68d170a1f68a2b94b99"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga8a6aec663c02f68d170a1f68a2b94b99">&#9670;&nbsp;</a></span>riscv_nn_softmax_s8_fast()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void riscv_nn_softmax_s8_fast </td>
          <td>(</td>
          <td class="paramtype">const q7_t *&#160;</td>
          <td class="paramname"><em>in_vec</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const uint16_t&#160;</td>
          <td class="paramname"><em>size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">q7_t *&#160;</td>
          <td class="paramname"><em>out_vec</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This function performs softmax calculations on signed 8-bit integer input vectors. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">in_vec</td><td>Pointer to the input vector </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">size</td><td>Number of elements in the input vector </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">out_vec</td><td>Pointer to the output vector </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>None</dd></dl>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>Outputs from inputs with very small values will be zero, which may cause the sum of the dequantized outputs to deviate from 1.</li>
<li>This is a 2-based softmax function.</li>
</ul>
</dd></dl>
<p><b>Example:</b> </p><div class="fragment"><div class="line"><span class="preprocessor">#define LENGTH 10</span></div>
<div class="line">q7_t in_data[LENGTH] = {...};</div>
<div class="line">q7_t out_data[LENGTH];</div>
<div class="line"> </div>
<div class="line"><a class="code" href="group__Softmax.html#ga8a6aec663c02f68d170a1f68a2b94b99">riscv_nn_softmax_s8_fast</a>(in_data, LENGTH, out_data);</div>
</div><!-- fragment --> 
</div>
</div>
<a id="gac4143da954d1f2ab48427022ef51c56e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gac4143da954d1f2ab48427022ef51c56e">&#9670;&nbsp;</a></span>riscv_nn_softmax_s16_fast()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void riscv_nn_softmax_s16_fast </td>
          <td>(</td>
          <td class="paramtype">const q15_t *&#160;</td>
          <td class="paramname"><em>in_vec</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const uint16_t&#160;</td>
          <td class="paramname"><em>size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">q15_t *&#160;</td>
          <td class="paramname"><em>out_vec</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This function performs softmax calculations on signed 16-bit integer input vectors. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">in_vec</td><td>Pointer to the input vector </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">size</td><td>Number of elements in the input vector </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">out_vec</td><td>Pointer to the output vector </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>None</dd></dl>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>Outputs from inputs with very small values will be zero, which may cause the sum of the dequantized outputs to deviate from 1.</li>
<li>This is a 2-based softmax function. </li>
</ul>
</dd></dl>

</div>
</div>
<a id="ga7cf0ecef9eef8cf2a773574d071abea7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga7cf0ecef9eef8cf2a773574d071abea7">&#9670;&nbsp;</a></span>riscv_nn_softmax_s8_hp()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void riscv_nn_softmax_s8_hp </td>
          <td>(</td>
          <td class="paramtype">const int8_t *&#160;</td>
          <td class="paramname"><em>in_tensor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int32_t&#160;</td>
          <td class="paramname"><em>in_tensor_row</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int32_t&#160;</td>
          <td class="paramname"><em>in_tensor_col</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int32_t&#160;</td>
          <td class="paramname"><em>scale</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int32_t&#160;</td>
          <td class="paramname"><em>lshift</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int32_t&#160;</td>
          <td class="paramname"><em>diff_min</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int8_t *&#160;</td>
          <td class="paramname"><em>out_tensor</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This function performs softmax calculations on signed 8-bit integer input/output tensors using a high-precision algorithm. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">in_tensor</td><td>Pointer to the input tensor </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">in_tensor_row</td><td>Number of rows in the input tensor </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">in_tensor_col</td><td>Number of columns in the input tensor </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">scale</td><td>Scaling value for input quantization </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">lshift</td><td>Left shift amount for input quantization </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">diff_min</td><td>Minimum threshold to perform the quantized exponential operation. The difference can be obtained by subtracting the input from the maximum in row. </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">out_tensor</td><td>Pointer to the output tensor </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>None </dd></dl>

</div>
</div>
<a id="ga1a4a512687ba8a9f42f5b416abd6903b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga1a4a512687ba8a9f42f5b416abd6903b">&#9670;&nbsp;</a></span>riscv_nn_softmax_s8_s16_hp()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void riscv_nn_softmax_s8_s16_hp </td>
          <td>(</td>
          <td class="paramtype">const int8_t *&#160;</td>
          <td class="paramname"><em>in_tensor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int32_t&#160;</td>
          <td class="paramname"><em>in_tensor_row</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int32_t&#160;</td>
          <td class="paramname"><em>in_tensor_col</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int32_t&#160;</td>
          <td class="paramname"><em>scale</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int32_t&#160;</td>
          <td class="paramname"><em>lshift</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int32_t&#160;</td>
          <td class="paramname"><em>diff_min</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int16_t *&#160;</td>
          <td class="paramname"><em>out_tensor</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This is a softmax function for signed 8-bit integer input tensor and signed 16-bit integer output tensor with high precision algorithm. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">in_tensor</td><td>Pointer to the input tensor </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">in_tensor_row</td><td>Number of rows in the input tensor </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">in_tensor_col</td><td>Number of columns in the input tensor </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">scale</td><td>Scaling value for input quantization </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">lshift</td><td>Left shift amount for input quantization </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">diff_min</td><td>Minimum threshold to perform the quantized exponential operation. The difference can be obtained by subtracting the input from the maximum in row. </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">out_tensor</td><td>Pointer to the output tensor </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>None </dd></dl>

</div>
</div>
<a id="gaeb8a9b8dd55434f4df1361ab320b8e9f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gaeb8a9b8dd55434f4df1361ab320b8e9f">&#9670;&nbsp;</a></span>riscv_nn_softmax_u8_hp()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void riscv_nn_softmax_u8_hp </td>
          <td>(</td>
          <td class="paramtype">const uint8_t *&#160;</td>
          <td class="paramname"><em>in_tensor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int32_t&#160;</td>
          <td class="paramname"><em>in_tensor_row</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int32_t&#160;</td>
          <td class="paramname"><em>in_tensor_col</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int32_t&#160;</td>
          <td class="paramname"><em>scale</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int32_t&#160;</td>
          <td class="paramname"><em>lshift</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int32_t&#160;</td>
          <td class="paramname"><em>diff_min</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint8_t *&#160;</td>
          <td class="paramname"><em>out_tensor</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This is a softmax function for unsigned 8-bit integer input tensor with high precision algorithm. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">in_tensor</td><td>Pointer to the input tensor </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">in_tensor_row</td><td>Number of rows in the input tensor </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">in_tensor_col</td><td>Number of columns in the input tensor </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">scale</td><td>Scaling value for input quantization </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">lshift</td><td>Left shift amount for input quantization </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">diff_min</td><td>Minimum threshold to perform the quantized exponential operation. The difference can be obtained by subtracting the input from the maximum in row. </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">out_tensor</td><td>Pointer to the output tensor </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>None </dd></dl>

</div>
</div>
<a id="ga1edc59ec814a19dc7055420c7831912d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga1edc59ec814a19dc7055420c7831912d">&#9670;&nbsp;</a></span>riscv_nn_softmax_s16_hp()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int riscv_nn_softmax_s16_hp </td>
          <td>(</td>
          <td class="paramtype">const int16_t *&#160;</td>
          <td class="paramname"><em>in_tensor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int32_t&#160;</td>
          <td class="paramname"><em>in_tensor_row</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int32_t&#160;</td>
          <td class="paramname"><em>in_tensor_col</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int32_t&#160;</td>
          <td class="paramname"><em>scale</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int32_t&#160;</td>
          <td class="paramname"><em>shift</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int16_t *&#160;</td>
          <td class="paramname"><em>exp_lut</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int16_t *&#160;</td>
          <td class="paramname"><em>one_by_one_lut</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int16_t *&#160;</td>
          <td class="paramname"><em>out_tensor</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This function performs softmax calculations on signed 16-bit integer input/output tensors using a high-precision algorithm. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">in_tensor</td><td>Pointer to the input tensor </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">in_tensor_row</td><td>Number of rows in the input tensor </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">in_tensor_col</td><td>Number of columns in the input tensor </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">scale</td><td>Scaling value for input quantization </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">shift</td><td>Left shift amount for input quantization </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">exp_lut</td><td>Pointer to the lookup table for exp(x), where x is uniformly distributed in [10, 0]. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">one_by_one_lut</td><td>Pointer to the lookup table for (1/(1+x)), where x is uniformly distributed in [0, 1]. </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">out_tensor</td><td>Pointer to the output tensor </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>This function only returns 0. </dd></dl>

</div>
</div>
<a id="gaafecfd2ae73cbb1fc175038dbd565204"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gaafecfd2ae73cbb1fc175038dbd565204">&#9670;&nbsp;</a></span>riscv_nn_softmax_f32()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int32_t riscv_nn_softmax_f32 </td>
          <td>(</td>
          <td class="paramtype">const float32_t *&#160;</td>
          <td class="paramname"><em>in_vec</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const uint32_t&#160;</td>
          <td class="paramname"><em>size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float32_t *&#160;</td>
          <td class="paramname"><em>out_vec</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This function performs softmax calculations on single-precision floating-point input vectors. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">in_vec</td><td>Pointer to the input vector </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">size</td><td>Number of elements in the input vector </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">out_vec</td><td>Pointer to the output vector </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>This function only returns 0. </dd></dl>

</div>
</div>
<a id="ga95f34f7382bc22ca80386f52e99624b7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga95f34f7382bc22ca80386f52e99624b7">&#9670;&nbsp;</a></span>riscv_nn_softmax_f32_2pass()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int32_t riscv_nn_softmax_f32_2pass </td>
          <td>(</td>
          <td class="paramtype">const float32_t *&#160;</td>
          <td class="paramname"><em>in_vec</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const uint32_t&#160;</td>
          <td class="paramname"><em>size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float32_t *&#160;</td>
          <td class="paramname"><em>out_vec</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This function performs softmax calculations on single-precision floating-point input vectors using a two-pass algorithm. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">in_vec</td><td>Pointer to the input vector </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">size</td><td>Number of elements in the input vector </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">out_vec</td><td>Pointer to the output vector </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>This function only returns 0. </dd></dl>

</div>
</div>
<a id="ga8ae830fce2d3b3a7ca70bac52828f6df"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga8ae830fce2d3b3a7ca70bac52828f6df">&#9670;&nbsp;</a></span>riscv_nn_softmax2d_f32()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int32_t riscv_nn_softmax2d_f32 </td>
          <td>(</td>
          <td class="paramtype">const float32_t *&#160;</td>
          <td class="paramname"><em>in_buf</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint32_t&#160;</td>
          <td class="paramname"><em>row</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint32_t&#160;</td>
          <td class="paramname"><em>col</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float32_t *&#160;</td>
          <td class="paramname"><em>out_buf</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float32_t *&#160;</td>
          <td class="paramname"><em>tmp_buf</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This function applies calculations to each row of a two-dimensional single-precision floating-point buffer. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">in_buf</td><td>Pointer to the input buffer </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">row</td><td>Number of rows in the two-dimension buffer </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">col</td><td>Number of columns in the two-dimension buffer </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">out_buf</td><td>Pointer to the output buffer </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">tmp_buf</td><td>Temporary buffer for calculations. It is required when -mext-vector is enabled and its size must be the same as input buffer. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>This function only returns 0. </dd></dl>

</div>
</div>
</div><!-- contents -->
</div><!-- doc-content -->
<div class="ttc" id="agroup__Softmax_html_ga8a6aec663c02f68d170a1f68a2b94b99"><div class="ttname"><a href="group__Softmax.html#ga8a6aec663c02f68d170a1f68a2b94b99">riscv_nn_softmax_s8_fast</a></div><div class="ttdeci">void riscv_nn_softmax_s8_fast(const q7_t *in_vec, const uint16_t size, q7_t *out_vec)</div><div class="ttdoc">This function performs softmax calculations on signed 8-bit integer input vectors.</div></div>
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.17 </li>
  </ul>
</div>
</body>
</html>
